{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\Git-Upload-Projects\\\\shipmentProject\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\Git-Upload-Projects\\\\shipmentProject'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import yaml\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationArticats:\n",
    "    transformed_object_file_path = str\n",
    "    transformed_train_file_path = str\n",
    "    transformed_test_file_path = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from shipment.logger import logging\n",
    "from pandas import DataFrame\n",
    "import numbers as np\n",
    "import pandas as pd\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "#from shipment.entity.config_entity import import DataTransformationConfig\n",
    "\n",
    "from shipment.entity.artifacts_entity import DataIngestionArtifacts\n",
    "\n",
    "from shipment.exception import shippingException\n",
    "\n",
    "from shipment.utils.main_utils import MainUtils\n",
    "from shipment.constant import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.UTILS = MainUtils()\n",
    "        self.SCHEMA_CONFIG = self.UTILS.read_yaml_file(filename=SCHEMA_FILE_PATH)\n",
    "        self.DATA_INGESTION_ARTIFACTS_DIR: str = os.path.join(\n",
    "            from_root(), ARTIFACTS_DIR, DATA_INGESTION_ARTIFACTS_DIR\n",
    "        )\n",
    "\n",
    "        self.DATA_TRANSFORMATION_ARTIFACTS_DIR: str = os.path.join(\n",
    "        from_root(), ARTIFACTS_DIR, DATA_TRANSFORMATION_ARTIFACTS_DIR\n",
    "        )\n",
    "        \n",
    "        self.TRANSFORMED_TRAIN_DATA_DIR: str = os.path.join(\n",
    "            self.DATA_TRANSFORMATION_ARTIFACTS_DIR, TRANSFORMED_TRAIN_DATA_DIR\n",
    "\n",
    "        )\n",
    "\n",
    "        self.TRANSFORMED_TEST_DATA_DIR: str = os.path.join(\n",
    "            self.DATA_TRANSFORMATION_ARTIFACTS_DIR, TRANSFORMED_TEST_DATA_DIR\n",
    "        \n",
    "        )\n",
    "\n",
    "        self.TRANSFORMED_TRAIN_FILE_PATH: str = os.path.join(\n",
    "            self.TRANSFORMED_TRAIN_DATA_DIR, TRANSFORMED_TRAIN_DATA_FILE_NAME\n",
    "        )\n",
    "\n",
    "        self.TRANSFORMED_TEST_FILE_PATH: str = os.path.join(\n",
    "            self.TRANSFORMED_TEST_DATA_DIR, TRANSFORMED_TEST_DATA_FILE_NAME\n",
    "        )\n",
    "\n",
    "        self.PREPROCESSOR_FILE_PATH = os.path.join(\n",
    "            from_root(),\n",
    "            ARTIFACTS_DIR,\n",
    "            DATA_TRANSFORMATION_ARTIFACTS_DIR,\n",
    "            PREPROCESSOR_OBJECT_FILE_NAME,\n",
    "        )    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_ingestion_artifacts = DataIngestionArtifacts,\n",
    "            data_transformation_config = DataTransformationConfig,\n",
    "    ):  \n",
    "        self.data_ingestion_artifacts = data_ingestion_artifacts\n",
    "        self.data_transformation_config = data_transformation_config\n",
    "\n",
    "        self.train_set = pd.read_csv(self.data_ingestion_artifacts.train_data_file_path)\n",
    "        self.test_set = pd.read_csv(self.data_ingestion_artifacts.test_data_file_path)\n",
    "    \n",
    "    def get_data_transformation_objects(self) -> object:\n",
    "        logging.info(\"Entered get_data_transformation_objects of DataIngection Class\")\n",
    "        try:\n",
    "            numerical_columns = self.data_transformation_config.SCHEMA_CONFIG[\"numerical_columns\"]\n",
    "            onehot_columns = self.data_transformation_config.SCHEMA_CONFIG[\"onehot_columns\"]\n",
    "            binary_columns = self.data_transformation_config.SCHEMA_CONFIG[\"binary_columns\"]\n",
    "            logging.info(\"got the numerical columns, onehot columns and binary columns from schema.yaml file\")\n",
    "\n",
    "            # lets create the transformation objects\n",
    "            numerical_transformer = StandardScaler()\n",
    "            oh_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "            binary_transformer = BinaryEncoder()\n",
    "            logging.info(\"Initialized StanderScaler, OneHotEncoder and BinaryEncoder\")\n",
    "\n",
    "            # Using transformer objects in column transformer\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    (\"OneHotEncoder\", oh_transformer, onehot_columns),\n",
    "                    (\"BinaryEncoder\", binary_transformer, binary_columns),\n",
    "                    (\"StandardScaler\", numerical_transformer, numerical_columns),\n",
    "                ]\n",
    "            )\n",
    "            logging.info(\"created the preprocessor object from column transformer\")\n",
    "            logging.info(\"Exited get_data_transformation_objects of DataTransformation class\")\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            raise shippingException(e, sys) from e\n",
    "\n",
    "    def _outlier__capping(col, df: DataFrame) -> DataFrame:\n",
    "        logging.info(\"Enterted the outerlier capping method of DtatTransformation class\")\n",
    "        try:\n",
    "            logging.info(\"Performing the outerlier capping for columns in dataframe\")\n",
    "            percentile25 = df[col].quantile(0.25)\n",
    "            percentile75 = df[col].quantile(0.75)\n",
    "\n",
    "            # fix the upper * lower bound\n",
    "            iqr = percentile75 - percentile25\n",
    "            upperlimit = percentile75 + 1.5 * iqr\n",
    "            lowerlimit = percentile25 - 1.5 * iqr\n",
    "\n",
    "            # capping the outliers\n",
    "            df.loc[(df[col] > upperlimit), col] = upperlimit\n",
    "            df.loc[(df[col] < lowerlimit), col] = lowerlimit\n",
    "            logging.info(\"Completed the outlier capping method of DataTransformation class\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise shippingException(e, sys) from e\n",
    "        \n",
    "    def initiate_data_transformation(self) -> DataTransformationArticats:\n",
    "        logging.info(\"Enteted the initial data transformation method of DataTransformation class\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self.data_transformation_config.DATA_TRANSFORMATION_ARTIFACTS_DIR, exist_ok=True)\n",
    "            logging.info(f\"created artifacts directory: for {os.path.basename(self.data_transformation_config.DATA_TRANSFORMATION_ARTIFACTS_DIR)}\")\n",
    "\n",
    "            preprocessor = self.get_data_transformation_objects()\n",
    "            logging.info(f\"got the peeprocessor object\")\n",
    "\n",
    "            target_column_name = self.data_transformation_config.SCHEMA_CONFIG[\"target_column\"]\n",
    "\n",
    "            numerical_columns = self.data_transformation_config.SCHEMA_CONFIG[numerical_columns]\n",
    "            logging.info(f\"got the target column name and numerical columns name: from schema config\")\n",
    "\n",
    "            # outlier capping\n",
    "            continuoues_columns = [\n",
    "                feature\n",
    "                for feature in numerical_columns\n",
    "                if len(self.train_set[feature].unique() >= 25)\n",
    "            ]\n",
    "            logging.info(f\"got a list of continuous column names\")\n",
    "            [self._outlier__capping(col, self.train_set) for col in continuoues_columns]\n",
    "            logging.info(f\"outlier capped in train df\")\n",
    "            [self._outlier__capping(col, self.test_set) for col in continuoues_columns]\n",
    "            logging.info(f\"outlier capped in test df\")\n",
    "            \n",
    "            # Getting the input features and target features of training dataset\n",
    "            input_feature_train_df = self.train_set.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_train_df = self.train_set[target_column_name]\n",
    "            logging.info(f\"got the train_x and train_y features from dataset\")\n",
    "\n",
    "            input_feature_test_df = self.test_set.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_test_df = self.test_set[target_column_name]\n",
    "            logging.info(f\"got the test_x and test_y features of from dataset\")\n",
    "            \n",
    "            # let apply the preprocessing object on training & testing datasets\n",
    "            input_feature_train_arr = preprocessor.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessor.transform(input_feature_test_df)\n",
    "            logging.info(f\"used the preprocessing object on training & test datasets to transform the features\")\n",
    "\n",
    "            #Concatinating the input feature array and target feature array of training dataset\n",
    "            train_arr = np.c[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            logging.info(f\"created the train array\")\n",
    "            \n",
    "            # creating the directory for transformed train dataset array and saving the transformed data\n",
    "            os.makedirs(self.data_transformation_config.TRANSFORMED_TRAIN_DATA_DIR, exist_ok=True)\n",
    "\n",
    "            transformed_train_file = self.data_transformation_config.UTILS.save_numpy_array_dataset(\n",
    "                self.data_transformation_config.TRANSFORMED_TRAIN_FILE_PATH, train_arr\n",
    "            )\n",
    "            logging.info(f\"saved train array to {os.path.basename.data_transformation_config.TRANSFORMED_ARTIFACTS_DIR}\")\n",
    "\n",
    "            #Concatinating the input feature array and target feature array of test dataset\n",
    "            test_arr = np.c[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            logging.info(f\"created the test array\")\n",
    "            \n",
    "            # creating the directory for transformed train dataset array and saving the transformed data\n",
    "            os.makedirs(self.data_transformation_config.TRANSFORMED_TEST_DATA_DIR, exist_ok=True)\n",
    "\n",
    "            transformed_test_file = self.data_transformation_config.UTILS.save_numpy_array_dataset(\n",
    "                self.data_transformation_config.TRANSFORMED_TEST_FILE_PATH, test_arr\n",
    "            )\n",
    "            logging.info(f\"saved train array to {os.path.basename.data_transformation_config.TRANSFORMED_ARTIFACTS_DIR}\")\n",
    "\n",
    "            preprocessor_obj_file = self.data_transformation_config.save_object(\n",
    "                self.data_transformation_config.PREPROCESSOR_FILE_PATH, preprocessor\n",
    "            )\n",
    "            logging.info(f\"saved the preprocessor object in DataTransformationArtifact directory\")\n",
    "            logging.info(f\"Exited to initate_date_transformation method of data_transformation class\")\n",
    "\n",
    "            # Saving data transformation artifacts\n",
    "            data_transformation_artifacts = DataTransformationArticats(\n",
    "                transformed_object_file_path = preprocessor_obj_file,\n",
    "                transformed_train_file_path = transformed_train_file,\n",
    "                transformed_test_file_path = transformed_test_file,\n",
    "            )\n",
    "            return data_transformation_artifacts\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise shippingException(e, sys) from e\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainPipeline:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "        \n",
    "    def start_data_transformation(\n",
    "            self, data_ingestion_artifacts: DataIngestionArtifacts\n",
    "     \n",
    "    ) -> DataTransformationArticats:\n",
    "        logging.info(\"Entered the start_data_transformation method of  TrainPipeline class\")\n",
    "\n",
    "        try:\n",
    "            data_transformation = DataTransformation(\n",
    "                data_ingestion_artifacts=data_ingestion_artifacts,\n",
    "                data_transformation_config=self.data_transformation_config,\n",
    "            )\n",
    "            data_transformation_artifacts = (\n",
    "                data_transformation.initiate_data_transformation()\n",
    "            )\n",
    "            logging.info(\"Exited to start_data_transformation method of TrainPipeline class\")\n",
    "            return data_transformation_artifacts\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise shippingException(e, sys) from e\n",
    "\n",
    "    def run_pipeline(self) -> None:\n",
    "        logging.info(\"Running the run_pipeline method of TrainPipeline class\")\n",
    "        try:\n",
    "            data_ingestion_artifact = self.start_data_ingestion()\n",
    "            data_validation_artifact = self.start_data_validation(\n",
    "                data_ingestion_artifact = data_ingestion_artifact\n",
    "            )\n",
    "            data_transformation_artifact = self.start_data_transformation(\n",
    "                data_ingestion_artifact = data_ingestion_artifact\n",
    "            )\n",
    "            logging.info(\"Exited the run_pipeline method of TrainPipeline class\")\n",
    "        except Exception as e:\n",
    "            raise shippingException(e, sys) from e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
